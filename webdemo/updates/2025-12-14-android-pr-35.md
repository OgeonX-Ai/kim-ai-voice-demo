# Handle HF client init lazily and harden upstream errors

![android](https://img.shields.io/badge/repo-android-blue)

**PR:** [#35](https://github.com/OgeonX-Ai/android/pull/35)\  
**Repo:** android\  
**Merged:** 2025-12-14T22:39:57Z\  
**Author:** OgeonX-Ai

## What changed
## Summary
- initialize the Hugging Face client lazily without conflicting model/base_url parameters and return a clear 502 when unavailable
- allow Whisper load failures to keep the backend alive and surface a 502 when STT is requested without a model
- wrap LLM calls with 502 error propagation so upstream proxy issues do not crash the endpoint

## Testing
- HF_API_TOKEN=dummy ELEVENLABS_API_KEY=dummy python -c "import main; print('import ok')"
- HF_API_TOKEN=dummy ELEVENLABS_API_KEY=dummy uvicorn main:app --host 0.0.0.0 --port 8000 (manual start)
- curl -X POST http://127.0.0.1:8000/talk -H "Content-Type: application/json" -d '{"text":"hello"}'


------
[Codex Task](https://chatgpt.com/codex/tasks/task_e_693f39c026d08325b12943b2f25c8f63)

## Impact
- +undefined / -undefined
- undefined files changed

## Tags
codex
